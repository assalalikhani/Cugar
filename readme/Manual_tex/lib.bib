@INPROCEEDINGS{Bie06a,
AUTHOR = {Biemann, C.},
TITLE = {Chinese Whispers - an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems},
BOOKTITLE = {Proceedings of the HLT-NAACL-06 Workshop on Textgraphs-06},
YEAR = {2006},
ADDRESS = {New York, USA}
}
@article{ ap,
    author = "Brendan J. Frey and Delbert Dueck",
    title = "Clustering by Passing Messages Between Data Points",
    journal = "Science",
    volume = "315",
    pages = "972--976",
    year = "2007",
    url = "www.psi.toronto.edu/affinitypropagation" }

@book{0,
    abstract = {An introduction to the practical application of cluster analysis, <i>Finding Groups in Data</i> presents a selection of methods that together can deal with most applications. These methods are chosen for their robustness, consistency, and general applicability. The text discusses the main approaches to clustering and provides guidance in choosing between the available methods. It also discusses various types of data, including interval-scaled and binary variables as well as similarity data and explains how these can be transformed prior to clustering. With numerous exercises to aid learning, <i>Finding Groups in Data</i> provides an invaluable introduction to cluster analysis with an emphasis on methods that are both easy to use and modern.},
    author = {Kaufman, Leonard and Rousseeuw, Peter J.},
    day = {25},
    howpublished = {Paperback},
    isbn = {0471735787},
    keywords = {cluster-analysis, clustering},
    month = {March},
    posted-at = {2007-04-20 01:24:35},
    priority = {2},
    publisher = {Wiley-Interscience},
    title = {Finding Groups in Data: An Introduction to Cluster Analysis (Wiley Series in Probability and Statistics)},
    url = {http://www.worldcat.org/isbn/0471735787},
    year = {2005}
}
@article{1,
 author = {Rousseeuw, Peter},
 title = {Silhouettes: a graphical aid to the interpretation and validation of cluster analysis},
 journal = {J. Comput. Appl. Math.},
 volume = {20},
 number = {1},
 year = {1987},
 issn = {0377-0427},
 pages = {53--65},
 doi = {http://dx.doi.org/10.1016/0377-0427(87)90125-7},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 }
}
@article{2,
author = {Jianbo Shi and Jitendra Malik},
title = {Normalized Cuts and Image Segmentation},
journal ={IEEE Transactions on Pattern Analysis and Machine Intelligence},
volume = {22},
issn = {0162-8828},
year = {2000},
pages = {888-905},
doi = {http://doi.ieeecomputersociety.org/10.1109/34.868688},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
}

@article{3,
    abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error<tex>R</tex>of such a rule must be at least as great as the Bayes probability of error<tex>R^{ast}</tex>--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the<tex>M</tex>-category case that<tex>R^{ast} leq R leq R^{ast}(2 --MR^{ast}/(M-1))</tex>, where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
    author = {Cover, T. and Hart, P.},
    citeulike-article-id = {995135},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1053964},
    journal = {Information Theory, IEEE Transactions on},
    keywords = {classification, nearest-neighbor, single-sided},
    number = {1},
    pages = {21--27},
    posted-at = {2006-12-14 17:27:18},
    priority = {2},
    title = {Nearest neighbor pattern classification},
    url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1053964},
    volume = {13},
    year = {1967}
}
@inproceedings{4,
  author = {Axel-Cyrille {Ngonga Ngomo} and Frank Schumacher},
  booktitle = {Proceedings of the 10th International Conference on Intelligent Text  Processing and Computational Linguistics (CICLING 2009)},
  interhash = {edb084fd7f3dfdd55a77784c20bf98c9},
  intrahash = {1a29b5013b7e5ee38b9ebc7fc7d3d162},
  note = {Best Presentation Award},
  pages = {547--558},
  title = {Border Flow – A local graph clustering algorithm for Natural Language  Processing},
  year = 2009,
  timestamp = {2009.12.17},
  keywords = {SIMBA group_aksw sys:relevantFor:bis sys:relevantFor:infai},
  added-at = {2010-07-07T16:14:25.000+0200},
  owner = {ngonga},
  biburl = {http://www.bibsonomy.org/bibtex/21a29b5013b7e5ee38b9ebc7fc7d3d162/aksw}
}
@phdthesis{5,
  author = {S. van Dongen},
  interhash = {bb59fbe628ad38a24133b3923112856e},
  intrahash = {6ad4ed5654e603ca06c758f5b7b3b6c9},
  school = {Centrum voor Wiskunde en Informatica},
  title = {A Cluster algorithm for graphs},
  url = {http://cat.inist.fr/?aModele=afficheN\&\#38;cpsidt=1409637},
  year = 2000,
  keywords = {clustering entityguides graph},
  added-at = {2009-03-12T15:42:50.000+0100},
  posted-at = {2009-03-10 07:53:13},
  location = {Amsterdam, Netherlands},
  priority = {2},
  biburl = {http://www.bibsonomy.org/bibtex/26ad4ed5654e603ca06c758f5b7b3b6c9/lillejul},
  citeulike-article-id = {4157198},
  abstract = {A cluster algorithm for graphs called the Markov Cluster algorithm (MCL algorithm) is introduced. The algorithm provides basically an interface to an algebraic process defined on stochastic matrices, called the MCL process. The graphs may be both weighted (with nonnegative weight) and directed. Let G be such a graph. The MCL algorithm simulates flow in G by first identifying G in a canonical way with a Markov graph G1. Flow is then alternatingly expanded and contracted, leading to a row of Markov Graphs G(i). Flow expansion corresponds with taking the kth power of a stochastic matrix, where k ∈ IN. Flow contraction corresponds with a parametrized operator Γr, r > 0, which maps the set of (column) stochastic matrices onto itself. The image ΓrM is obtained by raising each entry in M to the rth power and rescaling each column to have sum 1 again. The heuristic underlying this approach is the expectation that flow between dense regions which are sparsely connected will evaporate. The invariant limits of the process are easily derived and in practice the process converges very fast to such a limit, the structure of which has a generic interpretation as an overlapping clustering of the graph G. Overlap is limited to cases where the input graph has a symmetric structure inducing it. The contraction and expansion parameters of the MCL process influence the granularity of the output. The algorithm is space and time efficient and lends itself to drastic scaling. This report describes the MCL algorithm and process, convergence towards equilibrium states, interpretation of the states as clusterings, and implementation and scalability. The algorithm is introduced by first considering several related proposals towards graph clustering, of both combinatorial and probabilistic nature.}
}
@phdthesis{6,
  author = {Stijn van Dongen},
  interhash = {ca6f6746913ce2336e48391265860907},
  intrahash = {82b0c08517e642a5822b831508225133},
  school = {University of Utrecht},
  title = {Graph Clustering by Flow Simulation},
  year = 2000,
  timestamp = {2006.11.21},
  keywords = {imported},
  added-at = {2009-09-10T14:36:22.000+0200},
  owner = {gregor},
  biburl = {http://www.bibsonomy.org/bibtex/282b0c08517e642a5822b831508225133/gregoryy}
}

@inproceedings{7,
 author = {Shi, Jianbo and Malik, Jitendra},
 title = {Normalized Cuts and Image Segmentation},
 booktitle = {CVPR '97: Proceedings of the 1997 Conference on Computer Vision and Pattern Recognition (CVPR '97)},
 year = {1997},
 isbn = {0-8186-7822-4},
 pages = {731},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 }
@article{8,
    abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error<tex>R</tex>of such a rule must be at least as great as the Bayes probability of error<tex>R^{ast}</tex>--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the<tex>M</tex>-category case that<tex>R^{ast} leq R leq R^{ast}(2 --MR^{ast}/(M-1))</tex>, where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
    author = {Cover, T. and Hart, P.},
    citeulike-article-id = {995135},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1053964},
    journal = {Information Theory, IEEE Transactions on},
    keywords = {classification, nearest-neighbor, single-sided},
    number = {1},
    pages = {21--27},
    posted-at = {2006-12-14 17:27:18},
    priority = {2},
    title = {Nearest neighbor pattern classification},
    url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1053964},
    volume = {13},
    year = {1967}
}





